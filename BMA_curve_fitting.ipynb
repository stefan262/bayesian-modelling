{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BMA_curve_fitting.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "TzD0o2kEk3Lg"
      ],
      "authorship_tag": "ABX9TyM7aHYzNxuVncCHPR4R0+dx"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TzD0o2kEk3Lg"
      },
      "source": [
        "## import"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "17CA6_Xj8kH7"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import scipy.optimize\n",
        "import scipy.stats\n",
        "import math\n",
        "import random\n",
        "from sklearn import linear_model\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "from statsmodels.regression.linear_model import OLS\n",
        "from statsmodels.tools import add_constant\n",
        "from sklearn.model_selection import train_test_split\n",
        "from itertools import combinations\n",
        "\n",
        "from mpmath import mp\n",
        "mp.dps = 50"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m3pdF9nVk3zr"
      },
      "source": [
        "## class Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0kGyUUpKrQvF"
      },
      "source": [
        "class Model:\n",
        "  \n",
        "  def __init__(self,x_i,y_i,allowed_polynomials):\n",
        "    ''' class representing a simple polynomial regression model\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    x: 1d numpy.array\n",
        "        feature vector\n",
        "    y: 1d numpy.array\n",
        "        response vector\n",
        "    allowed_polynomials: 1d list\n",
        "        allowed degrees for polynomials of the feature vector\n",
        "    '''\n",
        "\n",
        "    self.x = x_i\n",
        "    self.y = y_i\n",
        "    self.allowed_polynomials = allowed_polynomials\n",
        "    self.n_param = len(allowed_polynomials)\n",
        "    self.likl=0\n",
        "    self.params = []\n",
        "\n",
        "    \n",
        "  def fit(self):\n",
        "    ''' fits the polynomial regression using ordinary least squares (OLS) from statsmodels.regression.linear_model \n",
        "    '''\n",
        "\n",
        "    model_features = np.column_stack([self.x**i for i in self.allowed_polynomials])\n",
        "    model_regr = OLS(self.y, model_features).fit()\n",
        "    self.likl = mp.exp(-model_regr.bic/2)\n",
        "    self.params = model_regr.params\n",
        "    \n",
        "    return self\n",
        "  \n",
        "  def is_subModel(self,other):\n",
        "    ''' check if a model is a subModel of another one\n",
        "    Parameters\n",
        "    ----------\n",
        "    other: Model\n",
        "        the other model that will be checked against\n",
        "    Returns\n",
        "    -------\n",
        "    True or False\n",
        "    '''\n",
        "\n",
        "    if len(self.allowed_polynomials) >= len(other.allowed_polynomials):\n",
        "      return False\n",
        "    else:\n",
        "      for i in range(0,len(self.allowed_polynomials)):\n",
        "        if not self.allowed_polynomials[i] in other.allowed_polynomials:\n",
        "          return False\n",
        "      return True\n",
        "  \n",
        "  def predict(self,x_i):\n",
        "\n",
        "    y_pred = np.zeros(len(x_i))\n",
        "    for i in range(0,len(x_i)):\n",
        "      for j in range(0, len(self.allowed_polynomials)):\n",
        "        y_pred[i]+= self.params[j] * np.power(x_i[i],self.allowed_polynomials[j])\n",
        "    \n",
        "    return y_pred\n"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R2BHNnof-97w"
      },
      "source": [
        "## class BMA_poly"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uiDl13ZD-9Xr"
      },
      "source": [
        "class BMA_poly:\n",
        "  \n",
        "  def __init__(self,x_i,y_i,max_degree,c_cutoff):\n",
        "    '''class repressenting Byaesian model averaging for polynomial curve fitting using the method known as \"Occam's window\". This implementation of Occam's window \n",
        "    rejects models that are 1/c_cutoff less likely than the most likely model, and rejects new complex models that are less likely than one of their submodels. \n",
        "    The priors are assumed to be uniform for this problem.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    x: 1d numpy.array\n",
        "        features vector\n",
        "    y: 1d numpy.array\n",
        "        response vector\n",
        "    max_degree: float\n",
        "        maximum degree for the polynomial regression (i.e. for max_degree = 10 we can have a polynomial of up to x^10)\n",
        "    c_cutoff: float\n",
        "        some models will be discarded if they predict the data far less well (i.e. if max_lik/lik_A > c_cutoff then model A will be discarded)\n",
        "    \n",
        "    '''\n",
        "\n",
        "    self.x = x_i\n",
        "    self.y = y_i\n",
        "    self.max_degree = max_degree\n",
        "    self.c_cutoff = c_cutoff\n",
        "    self.good_models=[]\n",
        "    self.sum_likl = 0\n",
        "\n",
        "  \n",
        "  def occam_razor(self, curr_model):\n",
        "    '''excludes complex models which receive less support from the data than their simpler counterparts.\n",
        "    Given the list of good_models is always ordered by the number of parameters all subModels will already be included and checked\n",
        "    Parameters\n",
        "    ----------\n",
        "    model: Model\n",
        "        model that needs to be checked\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    False if the model is too complex and True if the model is good and can be included\n",
        "    '''\n",
        "    for a_model in self.good_models:\n",
        "      if a_model.is_subModel(curr_model) and a_model.likl > curr_model.likl:\n",
        "        return False\n",
        "\n",
        "    return True \n",
        "\n",
        "  def fit(self):\n",
        "    '''perform the Bayesian model averaging \n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    self: BMA_poly\n",
        "        Returns the new BMA_poly object as an output\n",
        "    '''\n",
        "     \n",
        "    max_likl = 0\n",
        "    \n",
        "    # iterate through all possible models by going through the number of parameters (i.e. polynomial degrees)\n",
        "    for num_parameters in range(1,self.max_degree+1): \n",
        "\n",
        "      # list of all models with fixed num_parameters (it will be a list of numbers that represent the allowed polynomial degrees from 0->20)\n",
        "      current_list = list(combinations(list(range(self.max_degree)), num_parameters))\n",
        "\n",
        "      for allowed_polynomials in current_list:\n",
        "        curr_model = Model(self.x,self.y,allowed_polynomials).fit()\n",
        "        \n",
        "        # decide whether to save this model or not by first comparing its likl with the max_likl and then checking if there are any better subModels\n",
        "        if max_likl/curr_model.likl < self.c_cutoff and self.occam_razor(curr_model):\n",
        "          self.good_models.append(curr_model)\n",
        "\n",
        "          # update the max_likl if needed and update the list of good models\n",
        "          if curr_model.likl > max_likl:\n",
        "            max_likl = curr_model.likl\n",
        "            for a_model in self.good_models:\n",
        "              if max_likl/a_model.likl > self.c_cutoff:\n",
        "                self.good_models.remove(a_model)\n",
        " \n",
        "    self.sum_likl = 0\n",
        "    for a_model in self.good_models:\n",
        "      self.sum_likl += a_model.likl\n",
        "      \n",
        "    # Return the new BMA object as an output.\n",
        "    return self\n",
        "  \n",
        "  def predict(self, x_i):\n",
        "\n",
        "    y_pred = np.zeros(len(x_i))\n",
        "    for a_model in self.good_models:\n",
        "      if not a_model.likl/self.sum_likl < 1/1000:\n",
        "        prob = a_model.likl/self.sum_likl\n",
        "        for i in range(0,len(x_i)):\n",
        "          y_pred[i] +=  prob * a_model.predict(x_i)[i]\n",
        "\n",
        "    return y_pred\n"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6gVfqQm1_EQB"
      },
      "source": [
        "## tests"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h-6Dhgqu88Vy"
      },
      "source": [
        "# generate random data\n",
        "sigma = np.random.uniform(0,1) # constant noise\n",
        "num_poly = 11 # number of polyomials\n",
        "num_data = 200 # number of data points\n",
        "\n",
        "weights = [np.random.uniform(-1,1) for i in range(0,num_poly)]\n",
        "def f(x,weights):\n",
        "    y = np.random.uniform(0,1)\n",
        "    for i in range(0,len(weights)):\n",
        "        y += weights[i]*np.power(x,i)\n",
        "    return y\n",
        "\n",
        "# generate some input data and add noise\n",
        "input_data = [(lambda x:(x,f(x,weights)+sigma))(np.random.uniform(-5,5)) for i in range(0,num_data)]\n",
        "\n",
        "x_data, y_data = [[ i for i, j in input_data ],[ j for i, j in input_data ]] #unzip the tuples in input_data\n",
        "x_train = np.array(x_data)[:int(num_data/2)]\n",
        "y_train = np.array(y_data)[:int(num_data/2)]\n",
        "x_test = np.array(x_data)[int(num_data/2):]\n",
        "y_test = np.array(y_data)[int(num_data/2):]"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pmr9TDM6EiGk",
        "outputId": "0f8924df-b42d-46c7-96a3-49d6aa76bfec"
      },
      "source": [
        "result = BMA_poly(x_train, y_train, 13, 20).fit()\n",
        "for i in range(0,len(result.good_models)):\n",
        "  print(result.good_models[i].allowed_polynomials)\n",
        "  print(result.good_models[i].likl/result.sum_likl)\n"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(0, 1, 2, 3, 4, 6, 7, 8, 9, 10)\n",
            "0.012875195462866523192966957817522428828895950536037\n",
            "(0, 2, 3, 4, 5, 6, 7, 8, 9, 10)\n",
            "0.1842265391135412764411516452724461913739318532143\n",
            "(0, 2, 3, 4, 6, 7, 8, 9, 10, 11)\n",
            "0.03928166090473623231969436690272606872436863803638\n",
            "(0, 1, 2, 3, 4, 6, 7, 8, 9, 10, 12)\n",
            "0.048065155500610581453768404875635925647869855199006\n",
            "(0, 2, 3, 4, 5, 6, 7, 8, 9, 10, 12)\n",
            "0.59831666290412267852659465212467802287916529629311\n",
            "(0, 2, 3, 4, 6, 7, 8, 9, 10, 11, 12)\n",
            "0.056818403117858456261315014199967725188393624509425\n",
            "(0, 1, 2, 3, 4, 6, 7, 8, 9, 10, 11, 12)\n",
            "0.060416382996264251804508958807023637357374782211742\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LUhRaCpstIeM",
        "outputId": "85194d1a-405b-4910-f2c2-82bbfa930931"
      },
      "source": [
        "y_pred = result.predict(x_test)\n",
        "np.sqrt(sum((y_pred - y_test)**2)/len(y_test))\n",
        "# compare BMA_poly with the MAP method which only chooses the best among all models"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.2776149677955448"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rKb-b_DGTYUx",
        "outputId": "3f9fa059-6ca9-4285-9af2-2e959dacf058"
      },
      "source": [
        "np.sqrt(sum((y_pred - y_test)**2)/len(y_test))"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.289971024418702"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jICpTd1kBZxm"
      },
      "source": [
        ""
      ],
      "execution_count": 25,
      "outputs": []
    }
  ]
}